/**
 * â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
 * â”‚                   ğŸ“ Custom Tokenizer Implementation           â”‚
 * â”‚                                                                 â”‚
 * â”‚  Educational BPE (Byte-Pair Encoding) tokenizer built from     â”‚
 * â”‚  scratch to demonstrate tokenization concepts and algorithms.  â”‚
 * â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
 */

/**
 * â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 * ğŸ—ï¸ CLASS: CustomTokenizer
 * â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 *
 * Purpose: Educational implementation of Byte-Pair Encoding (BPE)
 *          to demonstrate how modern tokenizers work internally
 *
 * Features:
 *   â€¢ Character-level initialization
 *   â€¢ Iterative vocabulary building through BPE
 *   â€¢ Special token handling
 *   â€¢ Training from custom text corpora
 *   â€¢ Encoding/decoding with fallback handling
 *
 * Note: This is a simplified educational version. Production
 *       tokenizers use more sophisticated techniques.
 */
export class CustomTokenizer {
  /**
   * â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
   * â•‘ ğŸš€ CONSTRUCTOR: Initialize Empty Tokenizer                   â•‘
   * â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   *
   * Sets up the basic data structures needed for tokenization:
   * â€¢ Vocabulary mapping (token -> ID)
   * â€¢ Merge rules for BPE algorithm
   * â€¢ Special tokens for handling edge cases
   * â€¢ Decoder for reverse mapping (ID -> token)
   */
  constructor() {
    // â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    // â”‚ ğŸ“š Core Data Structures                                    â”‚
    // â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    this.vocab = new Map(); // token_string -> token_id
    this.merges = []; // BPE merge rules
    this.specialTokens = new Map(); // special_token -> token_id
    this.decoder = new Map(); // token_id -> token_string

    // ğŸ¯ Initialize with essential special tokens
    this.initializeSpecialTokens();
  }

  /**
   * â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
   * â•‘ â­ METHOD: initializeSpecialTokens                           â•‘
   * â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   *
   * Sets up special tokens that handle common edge cases:
   * â€¢ End of text markers
   * â€¢ Unknown token fallbacks
   * â€¢ Whitespace representations
   */
  initializeSpecialTokens() {
    const specials = [
      '<|endoftext|>', // ğŸ“„ Document/sequence terminator
      '<|startoftext|>', // ğŸ“ Document/sequence starter
      '<|unknown|>', // â“ Unknown/OOV token fallback
      '<|space|>', // âµ Explicit space representation
    ];

    // ğŸ·ï¸ Assign sequential IDs to special tokens
    specials.forEach((token, idx) => {
      this.specialTokens.set(token, idx);
      this.decoder.set(idx, token);
    });
  }

  /**
   * â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
   * â•‘ ğŸ“ METHOD: trainFromText - BPE Training Algorithm            â•‘
   * â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   *
   * Implements simplified Byte-Pair Encoding training:
   * 1. Initialize with character-level vocabulary
   * 2. Find most frequent character pairs
   * 3. Merge pairs iteratively to build subword vocabulary
   * 4. Continue until target vocabulary size reached
   *
   * @param {string} text - Training corpus
   * @param {number} vocabSize - Target vocabulary size
   */
  trainFromText(text, vocabSize = 8000) {
    console.log('ğŸ“ Training tokenizer...');

    // â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    // â”‚ ğŸ“ Phase 1: Character-Level Initialization                â”‚
    // â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    // Extract all unique characters plus common punctuation
    const chars = Array.from(new Set(text + ' \n\t!@#$%^&*()[]{}'));
    chars.forEach((char, idx) => {
      const tokenId = this.specialTokens.size + idx;
      this.vocab.set(char, tokenId);
      this.decoder.set(tokenId, char);
    });

    // â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    // â”‚ ğŸ”„ Phase 2: Iterative BPE Training                        â”‚
    // â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    let words = this.preprocessText(text);

    // Continue merging until we reach target vocab size
    for (let i = 0; i < Math.min(2000, vocabSize - this.vocab.size); i++) {
      // ğŸ“Š Find all character pairs and their frequencies
      const pairs = this.getPairs(words);
      if (pairs.size === 0) break;

      // ğŸ† Select the most frequent pair for merging
      const bestPair = this.getBestPair(pairs);
      if (!bestPair) break;

      // ğŸ”— Merge the selected pair across all words
      words = this.mergePair(words, bestPair);
      this.merges.push(bestPair);

      // ğŸ“š Add the new merged token to vocabulary
      const newToken = bestPair[0] + bestPair[1];
      const tokenId = this.vocab.size;
      this.vocab.set(newToken, tokenId);
      this.decoder.set(tokenId, newToken);
    }

    console.log(`âœ… Training complete! Vocabulary size: ${this.vocab.size}`);
  }

  /**
   * â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
   * â•‘ ğŸ§¹ METHOD: preprocessText - Text Preparation                 â•‘
   * â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   *
   * Prepares raw text for BPE training by:
   * â€¢ Normalizing whitespace
   * â€¢ Splitting into word units
   * â€¢ Filtering empty strings
   */
  preprocessText(text) {
    return text
      .replace(/\s+/g, ' ') // Normalize whitespace
      .split(' ') // Split on spaces
      .filter((word) => word.trim().length > 0); // Remove empty words
  }

  /**
   * â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
   * â•‘ ğŸ”¢ METHOD: encode - Text to Token IDs                        â•‘
   * â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   *
   * Converts input text to token ID array.
   * For simplicity, uses character-level encoding for better
   * visualization and educational purposes.
   *
   * @param {string} text - Input text to tokenize
   * @returns {number[]} Array of token IDs
   */
  encode(text) {
    if (!text) return [];

    const tokens = [];

    // ğŸ”¤ Character-based encoding for clear visualization
    for (const char of text) {
      const tokenId =
        this.vocab.get(char) || // Known character
        this.specialTokens.get('<|unknown|>') || // Unknown fallback
        2; // Default unknown ID
      tokens.push(tokenId);
    }

    return tokens;
  }

  /**
   * â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
   * â•‘ ğŸ”„ METHOD: decode - Token IDs to Text                        â•‘
   * â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   *
   * Reconstructs text from token ID array.
   *
   * @param {number[]} tokens - Array of token IDs
   * @returns {string} Reconstructed text
   */
  decode(tokens) {
    if (!Array.isArray(tokens)) return '';

    return tokens
      .map((token) => {
        const decoded = this.decoder.get(token);
        return decoded || '<|unknown|>';
      })
      .join('');
  }

  /**
   * â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
   * â•‘ ğŸ“Š METHOD: getPairs - Extract Character Pairs                â•‘
   * â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   *
   * Analyzes a word list to find all adjacent character pairs
   * and their frequencies. Essential for BPE algorithm.
   *
   * @param {string[]} words - List of words to analyze
   * @returns {Map} Pair frequencies (pair_string -> count)
   */
  getPairs(words) {
    const pairs = new Map();

    for (const word of words) {
      const chars = Array.from(word);

      // ğŸ” Extract all adjacent character pairs
      for (let i = 0; i < chars.length - 1; i++) {
        // Use separator to avoid pair collision (e.g., "ab" + "c" vs "a" + "bc")
        const pair = chars[i] + '|||' + chars[i + 1];
        pairs.set(pair, (pairs.get(pair) || 0) + 1);
      }
    }

    return pairs;
  }

  /**
   * â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
   * â•‘ ğŸ† METHOD: getBestPair - Find Most Frequent Pair             â•‘
   * â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   *
   * Identifies the character pair with highest frequency
   * for the next BPE merge operation.
   *
   * @param {Map} pairs - Pair frequency map
   * @returns {string[]|null} Most frequent pair or null if none
   */
  getBestPair(pairs) {
    if (pairs.size === 0) return null;

    let bestPair = null;
    let maxCount = 0;

    // ğŸ” Find pair with maximum frequency
    for (const [pair, count] of pairs) {
      if (count > maxCount) {
        maxCount = count;
        bestPair = pair.split('|||');
      }
    }

    return bestPair;
  }

  /**
   * â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
   * â•‘ ğŸ”— METHOD: mergePair - Apply BPE Merge Rule                  â•‘
   * â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   *
   * Applies a merge rule to all words, combining the specified
   * character pair wherever it appears.
   *
   * @param {string[]} words - Word list to process
   * @param {string[]} pair - Character pair to merge
   * @returns {string[]} Updated word list with merges applied
   */
  mergePair(words, pair) {
    const newWords = [];
    const [first, second] = pair;

    for (const word of words) {
      // ğŸ”„ Replace all occurrences of the pair with merged version
      const newWord = word.replace(
        new RegExp(this.escapeRegExp(first) + this.escapeRegExp(second), 'g'),
        first + second,
      );
      newWords.push(newWord);
    }

    return newWords;
  }

  /**
   * â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
   * â•‘ ğŸ›¡ï¸ METHOD: escapeRegExp - Regex Safety Utility              â•‘
   * â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   *
   * Escapes special regex characters to prevent injection issues.
   */
  escapeRegExp(string) {
    return string.replace(/[.*+?^${}()|[\]\\]/g, '\\$&');
  }

  // â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  // â”‚ ğŸ“Š Utility Methods for Analysis                           â”‚
  // â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  /**
   * Returns the current vocabulary size
   */
  getVocabSize() {
    return this.vocab.size;
  }

  /**
   * Returns a copy of the vocabulary for inspection
   */
  getVocab() {
    return new Map(this.vocab);
  }

  /**
   * Gets the text representation of a specific token ID
   */
  getTokenText(tokenId) {
    return this.decoder.get(tokenId) || '<|unknown|>';
  }
}
