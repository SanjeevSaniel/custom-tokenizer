/**
 * â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
 * â”‚                    ğŸ”§ TikToken Wrapper Class                   â”‚
 * â”‚                                                                 â”‚
 * â”‚  Elegant abstraction layer for OpenAI's tiktoken library      â”‚
 * â”‚  providing consistent tokenization across different AI models. â”‚
 * â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
 */

import { getEncoding, encodingForModel } from 'js-tiktoken';

/**
 * â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 * ğŸ›ï¸ CLASS: TiktokenWrapper
 * â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 *
 * Purpose: Provides a clean, consistent interface for tokenization
 *          operations across different OpenAI models
 *
 * Features:
 *   â€¢ Automatic model detection and fallback handling
 *   â€¢ Memory-safe resource management
 *   â€¢ Consistent API across all supported models
 *   â€¢ Graceful error handling and logging
 *   â€¢ Efficient encoding/decoding operations
 */
export class TiktokenWrapper {
  /**
   * â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
   * â•‘ ğŸš€ CONSTRUCTOR: Initialize Tokenizer                         â•‘
   * â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   *
   * Creates a new tokenizer instance for the specified model.
   * Implements intelligent fallback strategy for unsupported models.
   *
   * @param {string} modelName - Target AI model identifier
   *                           (e.g., 'gpt-4', 'gpt-3.5-turbo')
   */
  constructor(modelName = 'gpt-3.5-turbo') {
    try {
      // â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      // â”‚ ğŸ¯ Primary Strategy: Model-Specific Encoding           â”‚
      // â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      // Attempt to get the exact encoding for the requested model
      this.encoding = encodingForModel(modelName);
      this.modelName = modelName;
    } catch (error) {
      // â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      // â”‚ ğŸ›¡ï¸ Fallback Strategy: Universal Base Encoding          â”‚
      // â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      console.warn(
        `âš ï¸ Model ${modelName} not found, falling back to cl100k_base`,
      );

      // Use the most common base encoding as fallback
      // cl100k_base is used by GPT-4, GPT-3.5-turbo, and text-embedding models
      this.encoding = getEncoding('cl100k_base');
      this.modelName = 'cl100k_base';
    }
  }

  /**
   * â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
   * â•‘ ğŸ“ METHOD: encode - Text to Tokens                           â•‘
   * â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   *
   * Converts input text into an array of token IDs.
   *
   * @param {string} text - Input text to tokenize
   * @returns {number[]} Array of token IDs
   */
  encode(text) {
    // ğŸ›¡ï¸ Handle empty/null input gracefully
    if (!text) return [];

    // ğŸ”„ Convert Uint32Array to regular array for easier manipulation
    return Array.from(this.encoding.encode(text));
  }

  /**
   * â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
   * â•‘ ğŸ”„ METHOD: decode - Tokens to Text                           â•‘
   * â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   *
   * Converts an array of token IDs back into readable text.
   *
   * @param {number[]} tokens - Array of token IDs to decode
   * @returns {string} Reconstructed text
   */
  decode(tokens) {
    // ğŸ›¡ï¸ Validate input format
    if (!Array.isArray(tokens)) return '';

    // ğŸ”„ Convert to Uint32Array as required by tiktoken
    return this.encoding.decode(new Uint32Array(tokens));
  }

  /**
   * â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
   * â•‘ ğŸ” METHOD: getTokenText - Single Token Analysis              â•‘
   * â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   *
   * Retrieves the text representation of a single token ID.
   * Useful for token-by-token analysis and visualization.
   *
   * @param {number} tokenId - Individual token ID to decode
   * @returns {string} Text representation of the token
   */
  getTokenText(tokenId) {
    try {
      // ğŸ¯ Decode single token for inspection
      return this.encoding.decode(new Uint32Array([tokenId]));
    } catch (error) {
      // ğŸš« Return placeholder for invalid/unknown tokens
      return '<|unknown|>';
    }
  }

  /**
   * â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
   * â•‘ ğŸ“Š METHOD: getVocabSize - Vocabulary Information             â•‘
   * â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   *
   * Returns the approximate vocabulary size for the current model.
   * Note: This is an approximation as exact vocab sizes vary.
   *
   * @returns {number} Approximate vocabulary size
   */
  getVocabSize() {
    // ğŸ“ Standard vocab size for most OpenAI models
    return 100000; // Approximate vocab size
  }

  /**
   * â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
   * â•‘ ğŸ·ï¸ METHOD: getModelName - Model Identification               â•‘
   * â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   *
   * Returns the actual model/encoding name being used.
   * Useful for debugging and display purposes.
   *
   * @returns {string} Current model or encoding name
   */
  getModelName() {
    return this.modelName;
  }

  /**
   * â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
   * â•‘ ğŸ§¹ METHOD: free - Resource Cleanup                          â•‘
   * â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   *
   * Properly releases tokenizer resources to prevent memory leaks.
   * Should be called when the tokenizer is no longer needed.
   *
   * Critical for applications that create many tokenizer instances.
   */
  free() {
    // ğŸ§¹ Clean up encoding resources if available
    if (this.encoding && this.encoding.free) {
      this.encoding.free();
    }
  }
}
